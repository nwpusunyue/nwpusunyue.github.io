{"meta":{"title":"YueYue","subtitle":"孙悦的博客","description":"孙悦的博客|YueSun","author":"SunYue(孙悦)","url":"http://nwpusunyue.github.io"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-01-10T14:19:26.401Z","updated":"2018-01-10T14:19:26.401Z","comments":false,"path":"/404.html","permalink":"http://nwpusunyue.github.io//404.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-01-08T08:19:10.855Z","updated":"2018-01-08T08:19:10.855Z","comments":true,"path":"links/index.html","permalink":"http://nwpusunyue.github.io/links/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-01-08T08:19:10.855Z","updated":"2018-01-08T08:19:10.855Z","comments":false,"path":"categories/index.html","permalink":"http://nwpusunyue.github.io/categories/index.html","excerpt":"","text":""},{"title":"关于","date":"2018-01-10T14:22:00.382Z","updated":"2018-01-10T14:22:00.382Z","comments":false,"path":"about/index.html","permalink":"http://nwpusunyue.github.io/about/index.html","excerpt":"","text":"右边这些内容是模板上的，后期后逐渐丰富自己的项目经历，来完善自己的简介的，谢谢大家关注支持。"},{"title":"书单","date":"2018-01-08T08:19:10.854Z","updated":"2018-01-08T08:19:10.854Z","comments":false,"path":"books/index.html","permalink":"http://nwpusunyue.github.io/books/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-01-08T08:19:10.856Z","updated":"2018-01-08T08:19:10.856Z","comments":false,"path":"repository/index.html","permalink":"http://nwpusunyue.github.io/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-01-08T08:19:10.858Z","updated":"2018-01-08T08:19:10.858Z","comments":false,"path":"tags/index.html","permalink":"http://nwpusunyue.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"大数据分析方法:用分析驱动商业价值-1","slug":"大数据分析方法-用分析驱动商业价值-1","date":"2018-01-12T14:06:38.000Z","updated":"2018-01-13T15:59:11.392Z","comments":true,"path":"2018/01/12/大数据分析方法-用分析驱动商业价值-1/","link":"","permalink":"http://nwpusunyue.github.io/2018/01/12/大数据分析方法-用分析驱动商业价值-1/","excerpt":"","text":"序言 一周之前，我本来就是想培养自己阅读的习惯，顺便整理一些有用的电子版资料。在这样的想法之下，我决定搭建一个个人博客，没想到一弄就是三四天(Jekyll 模板是真的难以配置，尤其是在Windows下安装RubyGem工具及相关工具包，配置文件也看的不明不白，最后选用基于Node.js的Hexo模板，上手方便，一天就搭建好了)。废话不多说，开始积累我的阅读体验~~ 第1章 现代分析基本原则 文章摘抄 现在是数字媒体时代，也是web2.0、移动、云和大数据的时代。数据的容量、速率和多样性都呈爆发式增长。各大企业已经放弃使用单一数据仓库的理念，因为数据复杂的多样性已经让单一数据库很难驾驭。 本书提出了一个基于9项核心原则的办法： 实现商业价值和影响——构建并持续改进分析方法以实现高价值业务影响力。 专注于最后一英里——将分析部署到生产中，从而实现可复制的、持续的商业价值。 持续改善——从小处开始从而走向成功。 加速学习能力和执行力——行动、学习、适应、重复。 差异化分析——探索你的分析方法从而产生新的结果。 嵌入分析——将分析嵌入业务流程。 建立现代分析架构——利用通用硬件和下一代技术降低成本。 构建人力因素——培养并充分发挥人才潜力。 利用消费化趋势——利用不同的选择进行创新。 实现商业价值和影响 现代分析方法的原则之一就是聚焦分析那些具有潜在的改变组织游戏规则的价值的项目。 一些指标可以很容易识别和衡量，而其他指标在识别和衡量上有一定难度。为了发现这些潜在的指标，需要确定商业决策通常是由哪些因素决定的。首先要衡量这些因素的影响，然后有目的地建立对业务有直接影响的指标。 (如成熟的分析型组织通常会建立起兼顾资产负债两头的衡量标准——实现收益增长的同时必须有效控制成本。) 精明的企业可以通过 逆向思维 找到潜在的分析机遇，并且现实是往往那些在过去看来不可能解决的问题的壁垒早已不复存在。分析驱动的组织敢于打破条条框框，摆脱瓶颈，创造出大量的商业价值。通过 头脑风暴 寻求答案和新资源(数据，共生关系的合作者或技术)，不再使用样本和回溯测试来解决问题。 业务指标在“沙箱”分析模型(原始数据的一个有限子集、人工的、非生产的环境)下可能很好，但却在实际生产环境中表现得不尽如人意。所以，上线前，应该模拟实际的生产环境部署模型进行全面测试，对分析模型进行评估，而不是在理想的环境中评估。 专注最后一英里 终极目标：实现将分析部署到生产环境与承诺的为组织改变游戏规则的商业价值。清楚地认识到取得成功的代价，而不是如何取得成功。有了这样的认识后，为你的分析方法建立起量化的、远大的目标。 迅速完善并改进概念性验证(POC)或原型，使其可以进行生产部署。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://nwpusunyue.github.io/categories/读书笔记/"}],"tags":[{"name":"大数据分析","slug":"大数据分析","permalink":"http://nwpusunyue.github.io/tags/大数据分析/"}]},{"title":"新年蒙福","slug":"新年蒙福","date":"2018-01-12T01:58:32.000Z","updated":"2018-01-13T12:33:32.406Z","comments":true,"path":"2018/01/12/新年蒙福/","link":"","permalink":"http://nwpusunyue.github.io/2018/01/12/新年蒙福/","excerpt":"","text":"写在前面 *2018年，要更多寻求神，求主耶稣带领。* 下午，我在跟几位弟兄聊天的时候，说起读圣经的果效，有一些“不太好”的反馈。弟兄的一番话让我思考，到底自己的喜乐建立在什么方面之上？更多的价值和满足是来自于人生的“成功”光鲜，还是着眼于主耶稣的信实与平安？ 在这样的社会潮流之下，我该怎样找寻自己的价值？ 经文 123【哈3:17】虽然无花果树不发旺，葡萄树不结果，橄榄树也不效力，田地不出粮食，圈中绝了羊，棚内也没有牛；【哈3:18】然而，我要因耶和华欢欣，因救我的 神喜乐。【哈3:19】主耶和华是我的力量！他使我的脚快如母鹿的蹄，又使我稳行在高处。这歌交与伶长，用丝弦的乐器。 这一段经文本意是指迦勒底人入侵犹大而使山河俱废的事件(耶利米书6:6)。先知哈巴谷知道虽然犹大要遭受患难，但他却欢欣喜乐，因为他确信，他所仰赖依靠的那一位神是信实的真神，会在百姓中施行拯救。 我现在的处境远远好于当时的百姓，所以不要看周围环境和自己的软弱，要看到神的保守，更不要抱怨。依靠耶和华的人必大得力量，抵挡罪恶，过得胜的生活，稳行在高处。 祷告 元旦的时候，自己也为新的一年蒙神祝福祈求： 诗歌：《感谢神》 事项：数算主恩，感谢赞美 诗歌：《主，我邀请你》 事项：恢复关系，全心向主 诗歌：《诗篇23篇》 事项：个人一年的敬拜生活 诗歌：《万物的结局近了》《撒母耳啊快长》 事项：教会的发展 诗歌：《青年基督徒迅速兴起》 事项：传扬福音作见证 诗歌：《耶稣爱你》 事项：家人 诗歌：《弟兄姊妹们活在神家里》 事项：弟兄姊妹的生活、侍奉 诗歌：《我不知明天的道路》《常常喜乐》 事项；清心祷告、毕业、驾照、生活 诗歌：《所以要约束你们的心》 诗歌：《一粒麦子》 诗歌：《诗篇84篇》 感恩与赞美 敬拜的时候唱的诗歌： 《这里有神的同在》 《从岁首到年终》 《我今天为你祝福》 《感谢神》 默想 自己蒙受主耶稣深厚的恩典，需要细细数算并向神感恩称谢。还要更加刚强，结出喜乐的果子。以后读研了，相比于之前生活，事情会更多更杂，学术或者活动方面的成果会更少，但我要仰望我的神。(帖前5:16-18)要常常喜乐，不住地祷告，凡事谢恩，因为这是 神在基督耶稣里向你们所定的旨意。 最后，以一首诗歌结束吧，《你的目光要转向神》(mp3下载[转载自诗歌本])","categories":[{"name":"灵修笔记","slug":"灵修笔记","permalink":"http://nwpusunyue.github.io/categories/灵修笔记/"}],"tags":[{"name":"感恩","slug":"感恩","permalink":"http://nwpusunyue.github.io/tags/感恩/"},{"name":"赞美","slug":"赞美","permalink":"http://nwpusunyue.github.io/tags/赞美/"},{"name":"祷告","slug":"祷告","permalink":"http://nwpusunyue.github.io/tags/祷告/"}]},{"title":"Python爬取携程网站问答数据","slug":"python爬取携程网站问答数据","date":"2018-01-11T13:46:01.000Z","updated":"2018-01-13T12:34:24.329Z","comments":true,"path":"2018/01/11/python爬取携程网站问答数据/","link":"","permalink":"http://nwpusunyue.github.io/2018/01/11/python爬取携程网站问答数据/","excerpt":"","text":"【Tips：点击右下角锚链图标查看文章目录。】 背景 昨天师兄让我在携程上找一家酒店，抓取该酒店的问答信息。复制粘贴的操作繁琐枯燥，想学习用python爬虫来实现。果然，方法大似气力。经过简单入门的学习，仿写代码之后，终于实现了这个功能。 观察 从携程网站上打开一个酒店的网页，如珠海香江维克酒店：（可以留心看一下当前页面的URL，后续会用到） 在右侧的侧边栏中可以找到我们所需要的【酒店问答】模块：点击查看全部问答， 发现携程的评论分类是基于标签的,并且仔细观察，这个URL中出现的一串数字和首页出现的数字是一样的。 该页面是经过初步筛选的包含该酒店的问答信息。 每页有20条评论，详细的问答页需要再次点击查看，分别为： 思路 原本想参考网上博客《python爬虫（上）–请求——关于旅游网站的酒店评论爬取（传参方法）》的方法实现对json包的分析和提取，但是正如文章所提到的，携程网站做了反爬虫处理，无法通过分析Ajax传输的方式来获得网站数据。另外，携程技术中心研发经理崔广宇在中心官微发送的推文《干货|关于反爬虫，看这一篇就够了》中，也说明了携程网站针对反爬虫做了大量的工作：①每年三月份，学生大量爬数据造成的服务器负担问题，运维人员不得不设法维护；②研发人员针对爬虫做应对处理，如查封IP，禁止访问，甚至是构造假数据！ 从网页的network中查询到ajax传输数据的URL： 打开URL，发现并没有得到想要的json数据包，o(╥﹏╥)o 所以，正所谓功夫再高，也怕菜刀。不行我就暴力一些，就按照网页浏览的顺序进行： ·1 从全部问答信息中获得包含某酒店标签的问答详情页的URL ·2 从问答的详情页抓取提问的问题及回复的答案等数据 基础知识——正则表达式 正则表达式常用的符号 123456. 匹配换行符\\n除外的任意字符* 匹配前一个字符0次或无限次? 匹配前一个字符0词或1次.* 贪心算法(尽可能长的匹配).*? 非贪心算法(匹配到就认为成功，可多次匹配)() 括号内的数据作为结果返回 正则表达式的效果展示 123findall() 匹配所有符合规律的内容，返回包含结果的列表search() 匹配并提取第一个符合规律的内容，返回一个正则表达对象(object)sub() 替换符合规律的内容，返回替换后的值 正则表达式的实用技巧 使用findall() 和 search() 从文本中匹配感兴趣的内容 实用sub实现翻页功能 先抓大后抓小 实用\\d 匹配纯数字 代码实现 从全部问答信息中获得包含某酒店标签的问答详情页的URL (1)首先，我需要查看网页源代码，知道网页中每一条问答详情的超链接指向，以便正则匹配： 可以发现很明显的规律，就是包含酒店标签的都有共同的特征：data-href=&quot;/asks/zhuhai27/…html&quot;，所以要针对此项规律匹配即可。 (2)关于翻页的问题，第1页的url是：http://you.ctrip.com/asks/z5489970.html ， 第2页的url：http://you.ctrip.com/asks/z5489970-k3/p2.html，此后第n页就是pn.html，那第1页是否可以是p1.html呢？不妨一试，果然经过试验，p1.html 就是刚才的第一页！于是，可以推断出：5489970 是该酒店的特定hotelID,包含该酒店问答信息的网址为格式为：/zHotelID-k3/p#.html 所以按照这个格式进行翻页。 (3)代码 12345678910111213141516171819202122232425262728#-*-coding:utf-8*-## 获取到每个酒店的问答页import reimport requests# 携程原始评论old_url = 'http://you.ctrip.com/asks/z371230-k3/p1.html'total_page = 4 # 暂定为4，每页20条num = 0 # 为了保存所有的评论，num 设置为 全局for i in range(1,total_page+1): # sub 实现翻页 newlink = re.sub('/p\\d.html','/p%d.html'%i,old_url,re.S) # \\d 匹配纯数字 print newlink # 请求网页 html_obj = requests.get(newlink) html = html_obj.content # 匹配到问答数据网页地址 data-href=\"/asks/zhuhai27/4597548.html\" # zhuhai27 推测是城市及城市代码，北京地区为beijing1/ links = re.findall('data-href=\"(/asks/zhuhai27/........html)\"',html,re.S) for each in links: QALink = 'http://you.ctrip.com'+each # 构造网址 # print QALink print 'now saving: ' + QALink QAhtml_obj = requests.get(QALink) # 保存问答详情页到htmls下以便后续使用 fp = open('htmls\\\\'+str(num)+'.html','wb') fp.write(QAhtml_obj.content) fp.close() num += 1 (4) 结果 将详细的问答网页保存为html,保存在本地。 从问答的详情页抓取提问的问题及回复的答案等数据 (1) 检查网页元素，可以得到提问的问题的网页标签中class=“ask_title”, 回答的标签中class=“answer_text”,于是又可以匹配成功了。 (2) 代码 123456789101112131415161718192021222324252627#-*-coding:utf-8*-## 获取到html 后进行问答的爬取import reimport requestsimport os# 打开文件夹列出所有的文件path = './htmls/' files = os.listdir(path)# 保存问答信息到QA.txtQAInfo = open('QA.txt','w')num = 1 for file in files: # 打开文件 f = open(path+file,'r') html = f.read() f.close question = re.search('&lt;i class=\"icon_asktitle\"&gt;&lt;/i&gt;(.*?)\\n',html,re.S).group(1)# 保存问题 QAInfo.write('Q' + str(num) + \":\" + question+'\\n') answers = re.findall('&lt;p class=\"answer_text\"&gt;(.*?)&lt;/p&gt;',html,re.S) i=1 # 保存答案，答案有多个 for each in answers: QAInfo.write('\\t'+'A'+str(i)+':'+each+'\\n') i += 1 num+=1# 关闭文件QAInfo.close (3) 结果 得到问答数据QA.txt: 优化与改进 (1) 可以将两部分代码合并，不用保存在本地的过程，对两个程序进行合并操作。 (2) 可以将酒店的hotelID和地区(如zhuhai27)等信息剥离出来，作为输入项，可以批量实现对某地区某酒店问答信息的爬取。 (3) 正则匹配过于简单，以至于在结果中出现如空格&quot;&amp; nbsp&quot;;手机enjoy表情&quot;&amp;#128522; &quot;甚至是超链接的信息，需要进一步对正则表达式进行优化。 解决办法是：对于空格可以用beautifulsoup 中的 strip() 方法。对于enjoy 表情，可以参看博客《移动前端手机输入法自带emoji表情字符处理》处理。对于超链接信息，可以尝试获取标签的纯文本信息，先大定位，再小定位，用get_text()获取。 感想 实验半天，写博客半天，以后需要提高自己的码字速度和markdown语法的使用。 实验过程中，一开始不要想那么多细节，先把主要问题简单化，具体化，不要被细枝末节卡住，等到完成了基本功能之后，再逐步实现更优化的细节。这样效率会提高，而且也会有一步一步实现的满足感。 可以尝试一边做实验，一遍简单写博客(博客不是论文，太详细会浪费自己的时间。) ----------------------&lt;完&gt;---------------------- 附上合并后的代码： 12345678910111213141516171819202122232425262728293031323334353637#-*-coding:utf-8*-import reimport requests# 携程原始评论# 某酒店问答网址 'http://you.ctrip.com/asks/z430205-k3/p1.html'# 格式 430205 为特定的某酒店的HotelID p1 p2 为包含该酒店评论标签的评论页，每页20条old_url = 'http://you.ctrip.com/asks/z430205-k3/p1.html'total_page = 4# 为了保存所有的评论，num 设置为 全局QAInfo = open('QA-2.txt','w+')num = 1for i in range(1,total_page+1): # sub 实现翻页 newlink = re.sub('/p\\d.html','/p%d.html'%i,old_url,re.S) print newlink # 请求网页 html_obj = requests.get(newlink) html = html_obj.content # 匹配到问答数据网页地址 data-href=\"/asks/zhuhai27/4597548.html\" # 每页20条评论 links = re.findall('data-href=\"(/asks/zhuhai27/.*?.html)\"',html,re.S) for each in links: QALink = 'http://you.ctrip.com'+each # 构造网址 print QALink QAhtml_obj = requests.get(QALink) html = QAhtml_obj.content question = re.search('&lt;i class=\"icon_asktitle\"&gt;&lt;/i&gt;(.*?)\\n',html,re.S).group(1)# 保存问题 QAInfo.write('Q' + str(num) + \":\" + question+'\\n') print question answers = re.findall('&lt;p class=\"answer_text\"&gt;(.*?)&lt;/p&gt;',html,re.S) # 保存答案，答案有多个 i=1 for each in answers: QAInfo.write('\\t'+'A'+str(i)+':'+each+'\\n') i += 1 num+=1# 关闭文件QAInfo.close","categories":[{"name":"教程应用","slug":"教程应用","permalink":"http://nwpusunyue.github.io/categories/教程应用/"}],"tags":[{"name":"python","slug":"python","permalink":"http://nwpusunyue.github.io/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://nwpusunyue.github.io/tags/爬虫/"}]},{"title":"Welcome","slug":"welcome","date":"2018-01-08T09:29:32.000Z","updated":"2018-01-13T12:36:26.404Z","comments":true,"path":"2018/01/08/welcome/","link":"","permalink":"http://nwpusunyue.github.io/2018/01/08/welcome/","excerpt":"","text":"【Tips：点击右下角锚链图标查看文章目录。】 介绍 欢迎大家访问我的个人博客，我将更加努力！ 本博客参考hexo官网主题pure 分类 预期分为四大类,分别记录各种学习、生活内容： 灵修笔记 读书笔记 教程应用 闲谈杂记 目的 建立自己的个人博客，也是为了勉励自己日有所长，追赶潮流和大神脚步。 Hexo 过程 建站过程参考很多网页，并且也经历:jekyll一次次失败，再到安装配置好，再放弃，选用hexo模板的过程。 之后详细补充。 个人体会 使用Hexo建站十分的方便，好用。 只要你有兴趣，可以在几个小时之内搭建你自己的喜欢的博客。 折腾了好几天，终于有所收获。虽然现在还是对其原理一知半解，但是会在以后慢慢使用中更加熟悉了解。","categories":[{"name":"闲谈杂记","slug":"闲谈杂记","permalink":"http://nwpusunyue.github.io/categories/闲谈杂记/"}],"tags":[{"name":"welcome","slug":"welcome","permalink":"http://nwpusunyue.github.io/tags/welcome/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-01-08T08:14:06.924Z","updated":"2018-01-13T12:41:07.132Z","comments":true,"path":"2018/01/08/hello-world/","link":"","permalink":"http://nwpusunyue.github.io/2018/01/08/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","categories":[{"name":"教程应用","slug":"教程应用","permalink":"http://nwpusunyue.github.io/categories/教程应用/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://nwpusunyue.github.io/tags/hexo/"}]}]}